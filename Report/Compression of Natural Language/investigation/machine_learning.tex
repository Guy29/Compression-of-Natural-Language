\section{Machine learning-based compression}
\label{sec:machine_learning}

As discussed in the previous sections, any mechanism which can detect regularities in a stream of data and predict future observations (symbols) from past ones (context) can also be used for compression by using it to estimate the probabilities of potential future observations and using these to assign shorter codes to the most likely of them.

While n-gram language models are very simple and easy to train, they are limited by their pre-specified (and often short) context, namely the previous [n-1] symbols in a text, with symbols being either characters or full words. For this reason, statistical models such as n-gram language models have generally been superseded by neural network-based models such as recurrent neural networks (RNNs), long short-term memory models (LSTMs), and most recently large language models (LLMs) such as BERT and GPT, which can retain information across larger contexts and better capture long-range dependencies in a text.

For RNNs, this is accomplished by having the network's previous outputs act as additional inputs when it observes a new symbol, giving it a simple form of memory. An improvement on this model is LSTMs, which are a subtype of RNNs that can learn to select relevant information to keep within its memory, and which generally outperform simple RNNs. Lastly, transformer-based LLMs use an attention mechanism to weight the relative importance of all words in the input data when making a prediction.

In this section I perform simple experiments with one simple RNN and one LSTM, once again subclassing the \texttt{Predictor} class and plugging it (as well as an entropy coding) into a \texttt{Compressor}. The code for this section can be found at \texttt{\href{https://github.com/Guy29/FYP/blob/main/Code/machine_learning}{Code/machine\_learning}}.

As before, it is possible to feed the \texttt{decode} method noise to see what regularities the model has learned about.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{img/ML_decoding_randomness.png}
\caption{Using the \texttt{decode} method on a randomly generated input}
\label{fig:ML_decoding_randomness}
\end{figure}

With both models the text is much less coherent than with the n-gram model, although the models have still learned which byte values are characters, how often spaces are used, some common collocations, and in the case of the LSTM model some common words. It is likely that with more training or a wider context window the models would be better able to capture the regularities in English, but due to constraints on time and computational power it was not possible to test this in more depth.

As part of the process of training, my code serializes and saves the models periodically. One of the significant advantages of machine learning models over n-gram based models is that the trained version is much smaller (approximately 4 mb in the case of the LSTM and 1.5 mb in the case of the RNN).

My implementation of these models was unfortunately too slow to be practicable on large texts, although I suspect efficient implementations are possible. For an estimate of compression ratios, I use an excerpt from Sherlock Holmes, which neither model was trained on. This test indicates compression ratios of 1.253 and 2.008 for the RNN- and the LSTM-based compressors respectively.


\subsection{Discussion}

\todo[inline, color=yellow]{Mention autoencoders and latent spaces}

\todo[inline, color=yellow]{Write about potential use of LLMs, at least in future work subsection}