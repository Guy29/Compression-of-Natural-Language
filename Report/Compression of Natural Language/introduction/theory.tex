\section{Theory}
\label{sec:theory}

\subsection{Shannon}
\label{subsec:shannon}

In his seminal 1948 paper "A Mathematical Theory of Communication", Claude Shannon introduced the concept of information theoretic entropy \autocite{shannon1948mathematical}, defining it as the average amount of information provided by a stochastic source of data, given by the equation

\[H(X) = \mathbb{E}[-\log p(X)] = -\sum_{x \in \mathcal{X}} p(x) \log p(x)\]

for some discrete random variable X. In simple terms, the entropy of a variable is the average number of yes/no questions one needs to ask to determine the value of that variable. For a fair coin, one needs to ask only one yes/no question to learn the outcome, and so the entropy of that outcome is 1 bit (sometimes called 1 shannon).

For the English language, the entropy of individual letters based on their frequency is known to be approximately 4.14 bits. This can be obtained by calculating the negated sum of the products of the frequencies of the letters with the logs of those frequencies, in line with the formula above.

In his paper on the entropy of English text, Shannon improved on this, estimating that - when the letter is considered within its context - English contains 0.6 – 1.3 bits of information per letter \autocite{Shannon1951}. He arrives at this estimate by asking subjects to reconstruct a text by guessing its letters one by one, only being told whether their guess is correct or not, and by noting how well they perform at this task given a varying amount of context (1 to 100 previous letters). He finds that, when the subjects are given 100 previous letters for context, they are able to guess the next letter on their first try 80\% of the time. 

Shannon's analysis provides a theoretical best-case limit on how well natural language can be compressed. 

\subsection{Montague grammar}
In 1970, logician Richard Montague posited that there is “no important theoretical difference between natural languages and the artificial languages of logicians” and that it is “possible to comprehend the syntax and semantics of both kinds of language within a single natural and mathematically precise theory.” \autocite{Montague1970}

Montague gives a grammar which can be used to parse English, and a specification for a parser based on this grammar is given by Friedman \& Warren \autocite{Friedman1978}.